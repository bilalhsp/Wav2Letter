{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Speech2TextForConditionalGeneration, Speech2TextProcessor, Speech2TextModel\n",
    "from scipy import io\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = 'out_sentence_details_timit_all_loudness.mat'\n",
    "file_path = '/home/ahmedb/projects/Wav2Letter/data/'\n",
    "sentences = io.loadmat(os.path.join(file_path, mat_file), struct_as_record = False, squeeze_me = True, )\n",
    "features = sentences['features']\n",
    "phn_names = sentences['phnnames']\n",
    "sentdet = sentences['sentdet']\n",
    "fs = sentdet[0].soundf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44775,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav = sentdet[12].sound\n",
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/s2t-small-librispeech-asr were not used when initializing Speech2TextModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Speech2TextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Speech2TextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Speech2TextModel were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.encoder.embed_positions.weights', 'model.decoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-807b178be993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                 \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;31m# Create the position ids from the input token ids. Any padded tokens remain padded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         position_ids = self.create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length).to(\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "input_features = processor(wav, padding=True, sampling_rate=fs, return_tensors=\"pt\").input_features\n",
    "generated_ids = model(input_features)\n",
    "processor.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.encoder.conv.conv_layers.0.weight \t torch.Size([1024, 80, 5])\n",
      "model.encoder.conv.conv_layers.0.bias \t torch.Size([1024])\n",
      "model.encoder.conv.conv_layers.1.weight \t torch.Size([512, 512, 5])\n",
      "model.encoder.conv.conv_layers.1.bias \t torch.Size([512])\n",
      "model.encoder.embed_positions.weights \t torch.Size([6002, 256])\n",
      "model.encoder.layers.0.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.0.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.0.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.0.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.0.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.0.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.0.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.0.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.0.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.0.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.0.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.0.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.1.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.1.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.1.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.1.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.1.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.1.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.1.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.1.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.2.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.2.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.2.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.2.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.2.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.2.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.2.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.2.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.3.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.3.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.3.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.3.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.3.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.3.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.3.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.3.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.4.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.4.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.4.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.4.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.4.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.4.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.4.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.4.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.4.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.5.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.5.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.5.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.5.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.5.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.5.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.5.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.5.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.5.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.6.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.6.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.6.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.6.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.6.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.6.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.6.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.6.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.6.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.7.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.7.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.7.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.7.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.7.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.7.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.7.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.7.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.7.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.8.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.8.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.8.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.8.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.8.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.8.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.8.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.8.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.8.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.9.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.9.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.9.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.9.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.9.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.9.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.9.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.9.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.9.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.10.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.10.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.10.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.10.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.10.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.10.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.10.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.10.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.10.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.11.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.11.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.11.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.encoder.layers.11.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.fc1.weight \t torch.Size([2048, 256])\n",
      "model.encoder.layers.11.fc1.bias \t torch.Size([2048])\n",
      "model.encoder.layers.11.fc2.weight \t torch.Size([256, 2048])\n",
      "model.encoder.layers.11.fc2.bias \t torch.Size([256])\n",
      "model.encoder.layers.11.final_layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layers.11.final_layer_norm.bias \t torch.Size([256])\n",
      "model.encoder.layer_norm.weight \t torch.Size([256])\n",
      "model.encoder.layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.embed_tokens.weight \t torch.Size([10000, 256])\n",
      "model.decoder.embed_positions.weights \t torch.Size([1026, 256])\n",
      "model.decoder.layers.0.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.fc1.weight \t torch.Size([2048, 256])\n",
      "model.decoder.layers.0.fc1.bias \t torch.Size([2048])\n",
      "model.decoder.layers.0.fc2.weight \t torch.Size([256, 2048])\n",
      "model.decoder.layers.0.fc2.bias \t torch.Size([256])\n",
      "model.decoder.layers.0.final_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.0.final_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.fc1.weight \t torch.Size([2048, 256])\n",
      "model.decoder.layers.1.fc1.bias \t torch.Size([2048])\n",
      "model.decoder.layers.1.fc2.weight \t torch.Size([256, 2048])\n",
      "model.decoder.layers.1.fc2.bias \t torch.Size([256])\n",
      "model.decoder.layers.1.final_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.1.final_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.fc1.weight \t torch.Size([2048, 256])\n",
      "model.decoder.layers.2.fc1.bias \t torch.Size([2048])\n",
      "model.decoder.layers.2.fc2.weight \t torch.Size([256, 2048])\n",
      "model.decoder.layers.2.fc2.bias \t torch.Size([256])\n",
      "model.decoder.layers.2.final_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.2.final_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.fc1.weight \t torch.Size([2048, 256])\n",
      "model.decoder.layers.3.fc1.bias \t torch.Size([2048])\n",
      "model.decoder.layers.3.fc2.weight \t torch.Size([256, 2048])\n",
      "model.decoder.layers.3.fc2.bias \t torch.Size([256])\n",
      "model.decoder.layers.3.final_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.3.final_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.fc1.weight \t torch.Size([2048, 256])\n",
      "model.decoder.layers.4.fc1.bias \t torch.Size([2048])\n",
      "model.decoder.layers.4.fc2.weight \t torch.Size([256, 2048])\n",
      "model.decoder.layers.4.fc2.bias \t torch.Size([256])\n",
      "model.decoder.layers.4.final_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.4.final_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.self_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.self_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.self_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.self_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.self_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.self_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.self_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.fc1.weight \t torch.Size([2048, 256])\n",
      "model.decoder.layers.5.fc1.bias \t torch.Size([2048])\n",
      "model.decoder.layers.5.fc2.weight \t torch.Size([256, 2048])\n",
      "model.decoder.layers.5.fc2.bias \t torch.Size([256])\n",
      "model.decoder.layers.5.final_layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layers.5.final_layer_norm.bias \t torch.Size([256])\n",
      "model.decoder.layer_norm.weight \t torch.Size([256])\n",
      "model.decoder.layer_norm.bias \t torch.Size([256])\n",
      "lm_head.weight \t torch.Size([10000, 256])\n"
     ]
    }
   ],
   "source": [
    "for k, v in weights.items():\n",
    "    print(k, \"\\t\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech2TextForConditionalGeneration(\n",
      "  (model): Speech2TextModel(\n",
      "    (encoder): Speech2TextEncoder(\n",
      "      (conv): Conv1dSubsampler(\n",
      "        (conv_layers): ModuleList(\n",
      "          (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "          (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "        )\n",
      "      )\n",
      "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): Speech2TextEncoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): Speech2TextDecoder(\n",
      "      (embed_tokens): Embedding(10000, 256, padding_idx=1)\n",
      "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0): Speech2TextDecoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Speech2TextDecoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): Speech2TextDecoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): Speech2TextDecoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): Speech2TextDecoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): Speech2TextDecoderLayer(\n",
      "          (self_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): Speech2TextAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=10000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = list([*model.modules()])\n",
    "# mdls = OrderedDict([*model.named_modules()])\n",
    "encoder = mdls['model.encoder']\n",
    "decoder = mdls['model.decoder']\n",
    "head = mdls['lm_head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['', 'model', 'model.encoder', 'model.encoder.conv', 'model.encoder.conv.conv_layers', 'model.encoder.conv.conv_layers.0', 'model.encoder.conv.conv_layers.1', 'model.encoder.embed_positions', 'model.encoder.layers', 'model.encoder.layers.0', 'model.encoder.layers.0.self_attn', 'model.encoder.layers.0.self_attn.k_proj', 'model.encoder.layers.0.self_attn.v_proj', 'model.encoder.layers.0.self_attn.q_proj', 'model.encoder.layers.0.self_attn.out_proj', 'model.encoder.layers.0.self_attn_layer_norm', 'model.encoder.layers.0.fc1', 'model.encoder.layers.0.fc2', 'model.encoder.layers.0.final_layer_norm', 'model.encoder.layers.1', 'model.encoder.layers.1.self_attn', 'model.encoder.layers.1.self_attn.k_proj', 'model.encoder.layers.1.self_attn.v_proj', 'model.encoder.layers.1.self_attn.q_proj', 'model.encoder.layers.1.self_attn.out_proj', 'model.encoder.layers.1.self_attn_layer_norm', 'model.encoder.layers.1.fc1', 'model.encoder.layers.1.fc2', 'model.encoder.layers.1.final_layer_norm', 'model.encoder.layers.2', 'model.encoder.layers.2.self_attn', 'model.encoder.layers.2.self_attn.k_proj', 'model.encoder.layers.2.self_attn.v_proj', 'model.encoder.layers.2.self_attn.q_proj', 'model.encoder.layers.2.self_attn.out_proj', 'model.encoder.layers.2.self_attn_layer_norm', 'model.encoder.layers.2.fc1', 'model.encoder.layers.2.fc2', 'model.encoder.layers.2.final_layer_norm', 'model.encoder.layers.3', 'model.encoder.layers.3.self_attn', 'model.encoder.layers.3.self_attn.k_proj', 'model.encoder.layers.3.self_attn.v_proj', 'model.encoder.layers.3.self_attn.q_proj', 'model.encoder.layers.3.self_attn.out_proj', 'model.encoder.layers.3.self_attn_layer_norm', 'model.encoder.layers.3.fc1', 'model.encoder.layers.3.fc2', 'model.encoder.layers.3.final_layer_norm', 'model.encoder.layers.4', 'model.encoder.layers.4.self_attn', 'model.encoder.layers.4.self_attn.k_proj', 'model.encoder.layers.4.self_attn.v_proj', 'model.encoder.layers.4.self_attn.q_proj', 'model.encoder.layers.4.self_attn.out_proj', 'model.encoder.layers.4.self_attn_layer_norm', 'model.encoder.layers.4.fc1', 'model.encoder.layers.4.fc2', 'model.encoder.layers.4.final_layer_norm', 'model.encoder.layers.5', 'model.encoder.layers.5.self_attn', 'model.encoder.layers.5.self_attn.k_proj', 'model.encoder.layers.5.self_attn.v_proj', 'model.encoder.layers.5.self_attn.q_proj', 'model.encoder.layers.5.self_attn.out_proj', 'model.encoder.layers.5.self_attn_layer_norm', 'model.encoder.layers.5.fc1', 'model.encoder.layers.5.fc2', 'model.encoder.layers.5.final_layer_norm', 'model.encoder.layers.6', 'model.encoder.layers.6.self_attn', 'model.encoder.layers.6.self_attn.k_proj', 'model.encoder.layers.6.self_attn.v_proj', 'model.encoder.layers.6.self_attn.q_proj', 'model.encoder.layers.6.self_attn.out_proj', 'model.encoder.layers.6.self_attn_layer_norm', 'model.encoder.layers.6.fc1', 'model.encoder.layers.6.fc2', 'model.encoder.layers.6.final_layer_norm', 'model.encoder.layers.7', 'model.encoder.layers.7.self_attn', 'model.encoder.layers.7.self_attn.k_proj', 'model.encoder.layers.7.self_attn.v_proj', 'model.encoder.layers.7.self_attn.q_proj', 'model.encoder.layers.7.self_attn.out_proj', 'model.encoder.layers.7.self_attn_layer_norm', 'model.encoder.layers.7.fc1', 'model.encoder.layers.7.fc2', 'model.encoder.layers.7.final_layer_norm', 'model.encoder.layers.8', 'model.encoder.layers.8.self_attn', 'model.encoder.layers.8.self_attn.k_proj', 'model.encoder.layers.8.self_attn.v_proj', 'model.encoder.layers.8.self_attn.q_proj', 'model.encoder.layers.8.self_attn.out_proj', 'model.encoder.layers.8.self_attn_layer_norm', 'model.encoder.layers.8.fc1', 'model.encoder.layers.8.fc2', 'model.encoder.layers.8.final_layer_norm', 'model.encoder.layers.9', 'model.encoder.layers.9.self_attn', 'model.encoder.layers.9.self_attn.k_proj', 'model.encoder.layers.9.self_attn.v_proj', 'model.encoder.layers.9.self_attn.q_proj', 'model.encoder.layers.9.self_attn.out_proj', 'model.encoder.layers.9.self_attn_layer_norm', 'model.encoder.layers.9.fc1', 'model.encoder.layers.9.fc2', 'model.encoder.layers.9.final_layer_norm', 'model.encoder.layers.10', 'model.encoder.layers.10.self_attn', 'model.encoder.layers.10.self_attn.k_proj', 'model.encoder.layers.10.self_attn.v_proj', 'model.encoder.layers.10.self_attn.q_proj', 'model.encoder.layers.10.self_attn.out_proj', 'model.encoder.layers.10.self_attn_layer_norm', 'model.encoder.layers.10.fc1', 'model.encoder.layers.10.fc2', 'model.encoder.layers.10.final_layer_norm', 'model.encoder.layers.11', 'model.encoder.layers.11.self_attn', 'model.encoder.layers.11.self_attn.k_proj', 'model.encoder.layers.11.self_attn.v_proj', 'model.encoder.layers.11.self_attn.q_proj', 'model.encoder.layers.11.self_attn.out_proj', 'model.encoder.layers.11.self_attn_layer_norm', 'model.encoder.layers.11.fc1', 'model.encoder.layers.11.fc2', 'model.encoder.layers.11.final_layer_norm', 'model.encoder.layer_norm', 'model.decoder', 'model.decoder.embed_tokens', 'model.decoder.embed_positions', 'model.decoder.layers', 'model.decoder.layers.0', 'model.decoder.layers.0.self_attn', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.self_attn_layer_norm', 'model.decoder.layers.0.encoder_attn', 'model.decoder.layers.0.encoder_attn.k_proj', 'model.decoder.layers.0.encoder_attn.v_proj', 'model.decoder.layers.0.encoder_attn.q_proj', 'model.decoder.layers.0.encoder_attn.out_proj', 'model.decoder.layers.0.encoder_attn_layer_norm', 'model.decoder.layers.0.fc1', 'model.decoder.layers.0.fc2', 'model.decoder.layers.0.final_layer_norm', 'model.decoder.layers.1', 'model.decoder.layers.1.self_attn', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.self_attn_layer_norm', 'model.decoder.layers.1.encoder_attn', 'model.decoder.layers.1.encoder_attn.k_proj', 'model.decoder.layers.1.encoder_attn.v_proj', 'model.decoder.layers.1.encoder_attn.q_proj', 'model.decoder.layers.1.encoder_attn.out_proj', 'model.decoder.layers.1.encoder_attn_layer_norm', 'model.decoder.layers.1.fc1', 'model.decoder.layers.1.fc2', 'model.decoder.layers.1.final_layer_norm', 'model.decoder.layers.2', 'model.decoder.layers.2.self_attn', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.self_attn_layer_norm', 'model.decoder.layers.2.encoder_attn', 'model.decoder.layers.2.encoder_attn.k_proj', 'model.decoder.layers.2.encoder_attn.v_proj', 'model.decoder.layers.2.encoder_attn.q_proj', 'model.decoder.layers.2.encoder_attn.out_proj', 'model.decoder.layers.2.encoder_attn_layer_norm', 'model.decoder.layers.2.fc1', 'model.decoder.layers.2.fc2', 'model.decoder.layers.2.final_layer_norm', 'model.decoder.layers.3', 'model.decoder.layers.3.self_attn', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.self_attn_layer_norm', 'model.decoder.layers.3.encoder_attn', 'model.decoder.layers.3.encoder_attn.k_proj', 'model.decoder.layers.3.encoder_attn.v_proj', 'model.decoder.layers.3.encoder_attn.q_proj', 'model.decoder.layers.3.encoder_attn.out_proj', 'model.decoder.layers.3.encoder_attn_layer_norm', 'model.decoder.layers.3.fc1', 'model.decoder.layers.3.fc2', 'model.decoder.layers.3.final_layer_norm', 'model.decoder.layers.4', 'model.decoder.layers.4.self_attn', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.4.self_attn.out_proj', 'model.decoder.layers.4.self_attn_layer_norm', 'model.decoder.layers.4.encoder_attn', 'model.decoder.layers.4.encoder_attn.k_proj', 'model.decoder.layers.4.encoder_attn.v_proj', 'model.decoder.layers.4.encoder_attn.q_proj', 'model.decoder.layers.4.encoder_attn.out_proj', 'model.decoder.layers.4.encoder_attn_layer_norm', 'model.decoder.layers.4.fc1', 'model.decoder.layers.4.fc2', 'model.decoder.layers.4.final_layer_norm', 'model.decoder.layers.5', 'model.decoder.layers.5.self_attn', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.5.self_attn.out_proj', 'model.decoder.layers.5.self_attn_layer_norm', 'model.decoder.layers.5.encoder_attn', 'model.decoder.layers.5.encoder_attn.k_proj', 'model.decoder.layers.5.encoder_attn.v_proj', 'model.decoder.layers.5.encoder_attn.q_proj', 'model.decoder.layers.5.encoder_attn.out_proj', 'model.decoder.layers.5.encoder_attn_layer_norm', 'model.decoder.layers.5.fc1', 'model.decoder.layers.5.fc2', 'model.decoder.layers.5.final_layer_norm', 'model.decoder.layer_norm', 'lm_head'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdls.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = nn.Sequential(encoder, decoder, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedb/.local/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:559: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['it had gone like clockwork']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features = processor(wav, padding=True, sampling_rate=fs, return_tensors=\"pt\").input_features\n",
    "generated_ids = model.generate(input_features)\n",
    "processor.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e5a8e87b899d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;31m# Create the position ids from the input token ids. Any padded tokens remain padded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         position_ids = self.create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length).to(\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "copy(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value to be printed is : 010\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "print(f\"Value to be printed is : {epoch:03d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Anaconda 2020.11)",
   "language": "python",
   "name": "anaconda-2020.11-py38"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
